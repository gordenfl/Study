# Resolve Problem and Challenge

1. Tell me about a difficult bug you fixed.

* Game
    S - Situation
    While in the real-time game development, we started receiving some error log report from product env. it's about the discussion randomly in the battle session. The log does not show enough data to lets reproduce. It's random.
    T - Task
    As the backend engineer, it's my responsibility to identify the reason about this bug. and fix it as soon as possible.

    A - Action
    First, I analyzed the log across multi-server and notice that disconnects happened when players were moving and fight a lot of times in a short period.

    I add extra log and collect UPD package. After auto simulating a lot of time. we can reproduce this error happen. Through the log I added I find that the problem is cause the UDP package is not full package. it was truncated.

    After that, I rewrote the package generate logic and I also add a verify logic for that. then make a test on the simulate env. it does not happen again.

    R - Result
    After deploying the patch, the disconnection rate drop by over 90%. The fix also improved the experience in every kinds of network env. It told me that, while the data you packaged you must have a verify logic for it. keep the logic stable.

* Share Creators
    S - Situation
    While Sharecreators we implement the AI based on stablediffusion. The process is resource microservice send request to our AI module with Kafka. alway we can received the full request, but some times our stablediffusion can not receive the package resource microservice send.
    T - Task
    Because this is very important function in our platform. We need to keep our system stable. we need resolved this as soon as possible.

    A - Action
    I analyze all the log of all the data if full. And the AI module also can received all the message from Kafka. but the task is disappeared without any other reason. We check our AI Script, we find sometimes we call the AI, send all the description into stablediffusion will cause the GPU Overload.

    With analyze the stablediffusion code, because I am not so profession about AI, it's very hard for me. I just read a lot of article in the website, then I find that the problem is a flag is should set to True while GPU is overload, it will not send new request to GPU. It wil be in a queue to store all about that.

    R - Result
    After we fix this problem. our system becomes stable. User feedback improved.
    It was challenging but it let me learned a lot of how does stablediffusion works.

2. Describe a technical problem you solved creatively.

* Game product
    S - Situation
    In my multiplayer game projects,. We need a leaderboard tht could show real-time player rankings across different regions. However out infrastructure can not support that. And we do not have money to buy the distribute database clusters support that.

    T - Task
    I am a backend engineer. H had to design and implement a scalable, low-latency leaderboard system. It should handle millions of players updating their scores rapidly, only show Top-N rank in real time.

    A - Action
    I have an idea to using no-sql just Redis to help us. It's ZSET is sorted sets.
    * each region had its own ZSET in redis, where player ids were the members and scores were the values
    * use Redis' ZINCRBY to atomic update
    * For memory efficiency, I compressed the data and cache only 1000 players
    * To resolve the "Clod start" system I just use "Hybrid Storage": data is older than 7 days were flushed into a RDS achieved. and the background job refreshed Redis daily for active users.
    * exposed RESTfull API for query for querying individual ranks and the global top player. And add a delayed-write queue to batch data write  from Redis to SQL async.

    R - Result
    This design is served over 10million users, 99% of latency of under 50ms, and cost less than $100/month to run. It was stable even during peak traffic spikes. My team praised me and give me some gift for it.

* Share Creator
    S - Situation
    Our product has a performance issue: the AI generate picture with words description is too slow, because it will using the GPU and has a very long time to generate on that.

    T - Task
    As a experienced back engineer I need to optimize that make it mroe efficiency. I just read some article about the Stablediffusion and AI system.

    A - Action
    Instead to send request directly to GPU, I decide to add some argument to the request, and some people always want to see what's the description is right or not. the request is just a test, not a real generation.
    I just design a level for each request, default is low accuracy(精度), and if you need get real picture as your words description, just choose the hight accuracy to AI system. It can reduce GPU's load.

    R - Result
    After that improvement, the GPU does not so busy, and we reduced a lot of GPU spend. cost is reduced as well.


3. Tell me about a time when something broke in production. What did you do?

* Game Product
    S - Situation
    As a online multiplayer games, we have a very important function that is matchmaking system algorithm. Within a minute, player started reporting that they were stuck on the "Find Match...." screen. Could not find the match and enter into game.
    T - Task
    As the on-call backend engineer, and one of the core contributors to the matching service, I need to identify the root cause, rewrite the matching logic quick.
    A - Action
    First I checked the logs and system health log. and I saw the Matchmaker service was still receiving match requests not generate the game session.
    The matching throw exception and skip player record. That means the logic does not have valid matching.

    To fix that:
    * I try to revert this logic code to previous version through blue-green deployment switch.
    * Then create a hotfix to add a fallback default skill score for players
    * Add logs and metrics to matching catch silent skip in the feature queues.
    * Finally, I updated the test suite to include legacy player data.

    R - Result
    After the update, matchmaking system can provide the normal service. all the players are able to join the games again. the Queue became normal.
    After the incident, we add a backward-compatibility validation step to our release pipeline for all player data. It helps another kinds of problem such like this occur again.

* Share Creators
    S - Situation
    After publish the function of AI generation. we received a lot of warning report from the API response time even some of them timeout.
    T - Target
    Ad the lead of the whole engineer team. I was responsible for fix that issue. recover the service available and identifying the root reason of this problem.

    A - Action
    I started by looking into our APM tool and saw that our /api/metrics/report endpoint latency had jumped from 200ms to 3 seconds, and the CPU usage also increase at that time.
    After we find the point. I found new deploy feature add a optional query parameter to include detail event logs in the report. but the SQL query does not add this optimized logic. And it cause table scan on 10Million rows of that table, become so slow.
    We need fix this:
    * immediately rolled back this deployment to stop the pressure of DB
    * Collaborate with the developer who make the change to reactor the SQL to use indexes and limit result size.
    * Added a conditional logic to skip log joins. only permitted the logic who need that.
    * Test case include this logic

    R - Result
    After that we re-deployed the optimized version a few hours later.
    The incident let's add performance check in our CI pipeline 

4. Describe a performance issue you diagnosed and fixed.

* Game product
    S - Situation
    In our product of real-time game server, we noticed a sharp drop in server tick rate. and the latency during peek hours especial in larger PvP part. Players reported that movement and combat felt laggy.

    T - Task
    As the leader of Game server group. I just make a server-side simulation. my goal is to analyze the bottleneck of our server. and keep our frame rate recover to 50ms.
    
    A - Action
    profiling the server both on the engine(C++) and logic(Python). we find that when the PvP logic execute, we will create a lot of thread to deal with each fighting object, but we find that the player state sync logic performant reduce 60% of  tick time when there are 20+ player in one battlefield. 
    What I did:
    * Rerote ther serialization of the data we sync to each client. 
    * Using a priority-queue to manage the players status update package.
    * Moved all of real-time status work from Threading to async. I make a research of the Threading model we used, it's very low efficient then async. So we change the Multi-Thread model to async

    R - Result
    After update, tick time from 120+ to drop to 35. And this reduced input delay 90% and improved combat experience while multi-player fighting together.
    After that we extend this module of priority-queue to another part and don't use Python Threading anymore.
    